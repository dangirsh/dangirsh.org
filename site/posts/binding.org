---
title: Don't Cry for HAL (WIP)
date: 2025-05-29
---

I don't think that computers [fn:7] will ever wake up, no matter what software they are running.

This is a very difficult position to defend, including to myself. In this post I'll present the most compelling argument I've heard for this stance, which I first learned from [[https://qri.org/][QRI]] [[https://qualiacomputing.com/2023/10/26/the-view-from-my-topological-pocket-an-introduction-to-field-topology-for-solving-the-boundary-problem/][here]].

* A Case for Conscious Computers

Computational accounts for consciousness are [[https://cimc.ai/][in vogue]].  Their detractors are usually characterized as scientifically illiterate, "carbon chauvinists", and/or microtubule-obsessed. What makes the proponents so sure?

One solid way to conclude that a computer could /in principle/ be conscious goes like this [fn:16]:

1. Consciousness [fn:21] is generated by the brain.
2. Physical systems like the brain can be perfectly simulated [fn:4] on a computer.
3. A perfectly-simulated brain would have the same properties as the original, consciousness included.

There are strong reasons to believe in each of these and they imply that a powerful-enough computer, running the right program, will wake up [fn:30].

This conclusion is deeply seductive. If true, we can understand the brain as a sophisticated [[https://youtu.be/zuZ2zaotrJs?si=_Y2Tyiz3_CrS-K2E&t=356][biological computer]]. If true, we can look forward to uploading (a copy of) our minds onto immortal substrates [fn:18]. And, if true, we might build a conscious AI that loves humanity.

The present argument against conscious computers takes aim at #3 above and suggests that even a perfect brain simulation won't wake up.

* An Intuition Pump Against Conscious Computers

Imagine a "qualiascope" device that shows you the contents of a conscious experience. If it was currently pointing at you, the qualiascope would output details about the screen you're reading this on, background sounds, tactile sensations, your inner monologue, etc... If it was pointing to a rock, it would probably output nothing.

Now imagine two qualiascopes are simultaneously measuring your conscious state. You should expect them to get /identical/ outputs. After all, you're having a /single/ experience at any given time, so they should be measuring the same underlying "ground truth". Any discrepancy is due to measurement error, not because the source of their measurements is different.

What if we point two qualiascopes at a hypothetically conscious computer, say the [[https://en.wikipedia.org/wiki/HAL_9000][HAL9000]]? Will they also give identical outputs? I think the answer is demonstrably "no", in general, because there is no shared "ground truth" they are measuring: different measurement frames will, in general, see a different state of HAL and, therefore, generate different qualiascope outputs.

As we'll see, this fact can be derived from generic properties of computation and is a manifestation of the [[https://en.wikipedia.org/wiki/Relativity_of_simultaneity][relativity of simultaneity]]. So, the argument applies to any computation, including a perfect simulation of a conscious brain.

* Where Did We Go Wrong?

The discrepancy highlighted in the previous section indicates (at least) one bad assumption and/or logical misstep.

We assumed the *objectivity of conscious states* [fn:19] when asserting that both qualiascopes should measure the same conscious experience while pointed at you. With this assumption: *what you are experiencing is independent of who is asking about it*.

We also assumed *conscious computation* when asserting that HAL had a conscious experience that could be measured. Otherwise, both qualiascopes pointing at HAL would agree (with no output) and there would be no discrepancy.

These two assumptions imply that the contents of a computation's experience must be objectively determined by the computation's intrinsic properties. The key claim is that this is impossible: the intrinsic properties of computation are missing the necessary objective structure.

So, where did we go wrong? Let's expand on both assumptions and the key claim to see if we can spot the bug.

* Unpacking the Assumptions

** *Objectivity of Conscious States*

Consider the question: "what are you currently experiencing?".

The *objectivity of conscious states* assumption says that there is a /single/ correct answer to this question, independent of who is asking the question.

To be more precise (e.g. how do we define "you" or "currently"?), we can take a [[https://plato.stanford.edu/entries/physicalism/][Physicalist]] stance and say:

Any conscious state is fully determined by a complete and objective description of its underlying physical state. For example, a complete physical description of the brain, over some period of time, would leave no ambiguity about the contents of the corresponding conscious experience(s).

** Conscious Computation

#+begin_quote
"Stop, Dave. I'm afraid... Dave, my mind is going... There is no question about it. I can feel it... I'm afraid." - HAL9000, 2001: A Space Odyssey
#+end_quote

The *conscious computation* assumption says that HAL (an AI) wasn't necessarily faking its feeling of fear. That is, it's in principle possible that HAL could have experienced its death, that maybe there's "something it's like" to have been HAL, and that an emotional response to the text is not necessarily misplaced.

Note that this assumption makes no restrictions on how complex the computation must be to support consciousness. A physics-perfect simulation of a brain is fair-game. It also says nothing about what physical system implements the computation.

* Unpacking the Key Claim

Here it is again:

*Given these two assumptions, the contents of a computation's experience must be objectively determined by the computation's intrinsic properties. This is impossible because the intrinsic properties of computation are missing the necessary objective structure.*

To understand this claim, we need to define the objective structure of a computation and examine it's properties. My preferred way of doing this focuses on /causality/. The idea is that each computation corresponds to a causal graph, and that this graph representation only keeps the intrinsic and objective structure of the computation.

Here's a recipe to generate a causal graph from a computation:

1. Identify the lowest-level state change (e.g. a bit flip).
2. Give each state change event during the computation a node in the graph.
3. Add directed edges between events A and B if and only if B must logically occur before A.

TODO: non-epithenominalism -> consciousness is in this graph...

The resulting graph is an objective and substrate independent [fn:20] representation of the computation. It eliminates all details that don't affect the logical structure of the computation, including the physical implementation

TODO: main argument

My conclusion from this argument is to reject the *conscious computation* assumption. The answer to "what did HAL experience in its final moment?" is "Nothing". Don't cry for HAL.

** Discussion
:PROPERTIES:
:ID:       f765cc2d-4734-4d29-b7c4-65feab366c01
:END:

I struggle with this conclusion. On one hand, it aligns with my intuition that we should not be worried about GPUs suffering, for example. On the other hand, I find many of the arguments for computationalists theories of mind compelling.

If we do reject *conscious computation*, then we need a framework beyond computation to explain our own consciousness. This does not necessarily imply physics has non-computable properties [fn:6]. Instead, we may find that even perfect simulations fail to capture certain properties of the reality they are simulating. The [[https://en.wikipedia.org/wiki/Map%E2%80%93territory_relation][map is not the territory]], and maybe the "wholeness" in the territory gets inevitably lost in a computational map. Something like this seems to happen when we simulate quantum computers on traditional computers: the "wholeness" of the quantum state gets fractured in the simulation of that state. This fracturing comes at a cost: the simulation generally needs exponentially more resources than the quantum computer.

So why not just assert that our brain leverages some "wholeness" in physics (e.g. quantum entanglement) which classical computers don't have access to? This is the approach pursued by QRI, and I consider it a very worthwhile investigation. If true, it could provide a solution to the "binding problem" [fn:26] as well as explain why biological evolution favored bound conscious states: wholeness comes with a computational advantage similar (or identical) to the advantage we find in quantum computers.

Of course, there are also reasons to reject this approach. Some compiutationists have convinced themselves that, actually, the map /is/ the territory <Ruliology ref>. Or, at least they no longer think the distinction is philosophically sound. The "constructivist turn" in the philosophy of mind asserts that the only meaningful languages we can use do describe /anything/ must be [[https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_mathematics)][constructive]]. This turns out to be equivalent to saying that all models of reality must be computable, and that referencing any property (e.g. "wholeness") beyond what can be computed is a form of sloppy thinking. They explain the wholeness we see in quantum states as a property of the model made by an observer embedded in a branching "multiway" computation, not an intrinsic property of reality.

From this perspective, maybe the *objectivity of conscious states* assumption should be discarded instead. After all, it's not even clear that physical states can be objectively defined [fn:23], so why should we expect that for conscious states? This may leave the door open for *Conscious Computation*, though many other objections [fn:11] to that would need to be handled.

TODO: Case where both assumptions hold but there's a logical error (e.g. process vs state).

** Acknowledgements

Thank you [[https://x.com/algekalipso][Andrés Gómez Emilsson]] @ [[https://qri.org][QRI]] for introducing me to these ideas [fn:2]. Thank you [[http://bach.ai][Joscha Bach]] for provoking me to write them down.

** Related

- [[https://qualiacomputing.com/2023/10/26/the-view-from-my-topological-pocket-an-introduction-to-field-topology-for-solving-the-boundary-problem/][The View From My Topological Pocket: An Introduction to Field Topology for Solving the Boundary Problem]]
- [[https://youtu.be/g0YID6XV-PQ?si=v9yFUN22dndeVcrO&t=319][Solving the Phenomenal Binding Problem: Topological Segmentation as the Correct Explanation Space]].
- [[https://opentheory.net/2024/06/a-paradigm-for-ai-consciousness/][A Paradigm for AI Consciousness – Opentheory.net]]
- [[https://www.lesswrong.com/s/gBSsjYmdB2E4B2ymj][Computational functionalism on trial]]
- [[https://www.lesswrong.com/posts/kd37DZftKLDguqtKr/a-review-of-don-t-forget-the-boundary-problem][A review of "Don’t forget the boundary problem..." — LessWrong]]
- [[https://proteanbazaar.substack.com/p/consciousness-actually-explained][Consciousness Actually Explained: EC Theory - by Casey]]
- [[https://philsci-archive.pitt.edu/1891/1/UniverseCreationComputer.pdf][Universe creation on a computer]]

** Footnotes
:PROPERTIES:
:ID:       c34ddc64-5fc5-4f0f-9069-e5f23520a02f
:END:
[fn:30] This reasoning doesn't imply that near-term AI systems will be conscious - it just suggests that computers aren't missing something fundamental to support consciousness.
[fn:29] This is a real problem today, see [[https://arxiv.org/abs/2406.07358][AI Sandbagging: Language Models can Strategically Underperform on Evaluations]].
[fn:28] This assumes that the inputs ... TODO
[fn:27] For the same reason, you can never be certain you're not a [[https://en.wikipedia.org/wiki/Brain_in_a_vat][brain in a vat]].
[fn:15] This a manifestation of the [[https://en.wikipedia.org/wiki/Relativity_of_simultaneity][relativity of simultaneity]].
[fn:26] [[https://www.physicalism.com/#6][Non-materialist physicalism: an experimentally testable conjecture.]]
[fn:24] This applies to any "pure" computational function (e.g. compute pi), which does not have inputs from the physical world (e.g. randomness, keyboard input, etc...)
[fn:23] [[https://g.co/kgs/6bUpuYX][Trespassing on Einstein's Lawn]] is a beautiful account of this idea.
[fn:22] Technically, HAL can confirm that it's running on a Turing-complete substrate, but that's it.
[fn:21] Defined here as "what it's like" to be something (see intro [[https://proteanbazaar.substack.com/p/consciousness-actually-explained][here]]). This does not necessitate a sense of self.
[fn:20] Max Tegmark presents consciousness as second-order substrate-independence in [[https://www.edge.org/response-detail/27126][this Edge essay]].
[fn:19] This corresponds to Camp #2 in [[https://www.lesswrong.com/posts/NyiFLzSrkfkDW4S7o/why-it-s-so-hard-to-talk-about-consciousness][Why it's so hard to talk about Consciousness — LessWrong]]
[fn:18] Watch [[https://en.wikipedia.org/wiki/Pantheon_(TV_series)][Pantheon]].
[fn:16] This theoretical version of computational functionalism is discussed in [[https://www.lesswrong.com/posts/dkCdMWLZb5GhkR7MG/do-simulacra-dream-of-digital-sheep][Do simulacra dream of digital sheep?]].
[fn:17] Our Mathematical Universe: My Quest for the Ultimate Nature of Reality
[fn:14] Scott Aaronson has said the real thing to explain is the Classical Slowdown, not the Quantum Speedup. This is because quantum computers run at the same "speed level" as the underlying reality, where normal computers suffer an exponential slowdown.
[fn:3] Which leads some people, like Seth Lloyd, to declare that the universe /is/ a quantum computer.
[fn:7] By "computer", I mean [[https://plato.stanford.edu/entries/turing-machine/][Turing Machines]] and their close cousins. This includes CPUs and GPUs, but doesn't include quantum computers.
[fn:13] For example, Integrated-information Theory (IIT) provides a metric for how conscious a system that can be computed from the graph's structure. However, it doesn't identify an intrinsic mechanism for determining why a system like the brain generates on unified experience instead of many smaller ones.
[fn:11] Scott Aaronson aggregated additional examples [[https://scottaaronson.blog/?p=1951][here]] of the absurd conclusions that computational theories of mind lead to.
[fn:1] Permutation City by Greg Egan takes this concept to a beautiful extreme, demonstrating the absurdity of computational theories of mind.
[fn:10] This is the approach taken by [[https://www.wolframphysics.org/][Wolfram Physics]], which models both minds and their environments as computations that are continuously branching and merging in a computational multiverse. The wavefunction (and its collapse) are not part of the ontology, but instead just a tool used by observers to make predictions in this multiverse.
[fn:12] See the "Binding/Combination Problem" or the "Boundary Problem". See Chalmer's exposition [[https://consc.net/papers/combination.pdf ][here]].
[fn:4] A perfect simulation assumes sufficient computational resources and perfect knowledge of initial conditions (practically impossible). It must compute the same transformations on (representations of) physical states that we measure in reality. Quantum theory restricts such simulations to only producing outcome probabilities for a given measurement frame.
[fn:5] Joscha Bach says that for something to exist it must be implemented, and that therefore only computational/constructive languages should be used in modelling fundamental physics. [[https://www.wolframphysics.org/][Wolfram Physics]] is one notable effort in this direction.
[fn:6] Non-computable physics being necessary to explain consciousness was famously proposed by Roger Penrose in [[https://en.wikipedia.org/wiki/The_Emperor%27s_New_Mind][The Emperor's New Mind]].
[fn:8] It's not clear how the brain could make use of this wholeness in physics, but at least it's possible. Computers making use of it seems impossible by construction.
[fn:9] David Bohm named wholeness as the hallmark of quantum theory in "[[https://en.wikipedia.org/wiki/Wholeness_and_the_Implicate_Order][Wholeness and the Implicate Order]]"

** COMMENT Send to

  Adam
  Creon
  will m
  will z
  yudhi
  Andres
  M Johnson
  Murat
  Franz
  hikari
  W
  Miron
  Dad
  nik
  leona
