---
title: Don't Cry for HAL (WIP)
date: 2025-05-29
---

I don't think that computers[fn:1] will ever wake up, no matter what software they are running. In this post, I'll present my version of a lesser-known argument[fn:3] for this stance, which I first learned from [[https://qri.org/][QRI]] [[https://qualiacomputing.com/2023/10/26/the-view-from-my-topological-pocket-an-introduction-to-field-topology-for-solving-the-boundary-problem/][here]][fn:4].

The main idea focuses on an aspect of consciousness[fn:2] called "global binding"[fn:5], which is the deceptively simple observation that we are /simultaneously/ aware of the contents of our experience. In other words, we have a experiential "now" with rich structures in it. To make my point, I'll only need to use a well-defined simplification of global binding: the simultaneous experience of two events.

I'll show that, under certain reasonable assumptions about consciousness, the framework of computation cannot account for global binding. I then discuss the implications of this conclusion, including the option of questioning the assumptions.

I try to assume minimal background knowledge in this post, since I believe this argument merits wider awareness.

* A Case for Conscious Computers

Computational accounts for consciousness are [[https://cimc.ai/][in vogue]].  Their detractors risk being characterized as scientifically illiterate, carbon chauvinists[fn:19], or hyperfixated on microtubules[fn:20]. What makes the proponents so sure that consciousness is a property of computation?

One solid way to conclude that a computer could /in principle/ be conscious goes like this[fn:6]:

1. Consciousness is generated by the brain.
2. Physical systems like the brain can be perfectly simulated[fn:7] on a computer.
3. A perfectly-simulated brain would have the same properties as the original, consciousness included.

There are strong reasons to believe in each of these and they imply that a powerful-enough computer, running the right program, will wake up[fn:8].

This conclusion is deeply seductive to technologists. If true, we can understand the brain as a sophisticated biological computer[fn:22]. If true, we may one day upload our minds onto immortal computational substrates[fn:9]. And, if true, we might build a conscious AI that's a worthy successor to humanity[fn:21]. The stakes couldn't be higher!

The present argument against conscious computers takes aim at #3 above and suggests that no computation, not even a perfect brain simulation, can wake up.

* What Did HAL See?
:PROPERTIES:
:ID:       23843b1e-f10e-4f28-8a29-8519e46310ab
:END:

Let's start with a thought experiment to make things more concrete.

Imagine a device called a "qualiascope"[fn:24] that shows you the contents of a conscious experience (i.e. it's a mind reader). If it were currently pointing at you, the qualiascope would output details about the screen you're reading this on, your inner monologue, background sounds, etc... If it were pointing at a rock, it would probably output nothing[fn:23].

Two bolts of lightning flash before your eyes, and you experience them as /simultaneous./ Any qualiascope pointing at you during that time should agree and output something like: "simultaneous flashes".

Two more strikes, but this time you perceive a small gap in time between them. Now any qualiascope pointing at you should say: "back-to-back flashes".

The stikes continue many times, and any qualiascope pointing at you always accurately matches your experience.

#+CAPTION: You experience two flashes of lightning simultaneously, how lucky!
#+ATTR_HTML: :style width:60%;max-width:60%;margin-left:auto;margin-right:auto;display:block
[[../img/theda-lightning.jpeg]]

Everything seems simple so far, but we've already snuck in a miracle! *What could the qualiascopes be measuring to always agree with your experience?* Is such a device even physically possible?

Your first guess may be that qualiascopes measure some relevant events in your brain (e.g. neurons firing) and then infer the simultaneity of your experience based on the simultaneity of these events. However, simultaneity is only defined relative to a measurement frame[fn:17]. What's right frame to use here? What's more, we should expect the fact of whether the strikes are simultaneous (or not) /in your conscious experience/ to be independent of the measurement frame. After all, what you're experiencing is independent of who is asking about it! This reasoning makes the simulteneity of brain (or any other) events a dubious candidate for the source of the qualiascopes' measurements.

While you ponder that, let's consider repeating the lightning scenario with a purportedly conscious computer named "HAL"[fn:31] in your place. After the each pair of strikes, HAL shows an output indicating if it experienced the strikes as simultaneous or not. A qualiascope pointing at HAL does the same. *Is it possible for the qualiascope to always agree with HAL?*

Now, we're on more solid footing: unlike the brain, there is no mystery about HAL's mechanism. We can understand what the qualiascope has access to within the framework of computation. We'll find that there's no way to account for the /objectivity/ of HAL's purported conscious experience of simulteneity using HAL's computational structure alone.  This leaves the door open for each qualiascope to do it differently: one qualiascope may output "synchronous lightning strikes" and the other "back-to-back lightning strikes" and /neither/ is more correct!

* Proceeding with Care
:PROPERTIES:
:ID:       f71b4bba-06be-4542-865d-1071581a82ed
:END:

The discrepancy in our thought experiment calls our assumptions into question, including the assumption that HAL was conscious in the first place. Before jumping to a conclusion like "therefore computers can't be conscious", lets carefully lay out all of the key assumptions, claims, and logical steps:

The primary *assumptions about consciousness* we used are:

1.1. The contents of conscious states can be reported by a hypothetical qualiascope.

1.2. The contents of conscious states are objective, meaning all qualiascopes measuring the same state must agree.

1.3. The contents of conscious states are objectively grouped into moments of experience, based on subjective simultaneity.

We additionally rely on these *claims about computation*[fn:26]

2.1. Some computations correspond to conscious states[fn:10].

2.2. Computations have an objective structure that is the source of the qualiascopes' measurements.

2.3. The objective structure of computation cannot inform the grouping of assumption 1.3.

Here are the *logical steps for the contradiction*:

Following 1.1 and 2.1, we point a qualiascope at a conscious computer and expect some output. Given 1.2, we do the same with two qualiascopes and expect matching outputs. From 1.3, we know these matching outputs must always agree on which parts of the computer's experience are grouped into the same "moment" (e.g. synchronous lightning strikes). This implies the "ground truth" of this grouping is present in the objective structure of the computation (2.2) being measured, otherwise, on what basis can we expect all qualiascopes to agree? Finally, claim 2.3 directly contradicts this implication.

For now, I'm going to take all of these assumptions as given and focus on explaining claim 2.3. Then, I'll review ways of resolving this contradiction, including my preferred choice of rejecting 2.1 (i.e. computers can't be conscious) as well as questioning the other assumptions.

* Distilling Computation to Causal Graphs
:PROPERTIES:
:ID:       1fd6971d-ed16-4634-b0a3-1fa7eed3fc90
:END:

To understand 2.3, we need to first define the objective structure of a computation (2.2) that is relevant for its conscious state. This is tricky because, consciousness aside, it's not obvious how to think about a computation's structure: a function can be computed by different algorithms (e.g. bubble or merge sort), algorithms have multiple implementations (e.g. serial or parallel), and these implementations can run on many different physical substrates (e.g. silicon or [[https://www.youtube.com/watch?v=vo8izCKHiF0][wood]])[fn:11].

From assumption 1.1, we can infer that conscious states must participate in /causality/. Otherwise, they could not be measured by causally affecting the output of a qualiascope. This suggests using the /causal structure/ of a computation as the relevant representation for 2.2. If there's some aspect of a computation not captured by its causal structure, then /by definition/ it can't affect the output of the qualiascope and is therefore irrelevant under the present assumptions about consciousness.

Another way to arrive at causal structure is to take an /internal perspective/ of a computation. Imagine an AI exploring a self-contained virtual world. What can it learn, in principle, about its situation? Very simply, it can take actions, record observations, and build predictive models. Critically, these will never reveal, for example, if it's running on a CPU or GPU. That's because it can /only infer the causal structure of its virtual world/, which could be physically implemented in many different ways. The same argument applies to an AI building models of its own consciousness: only the causal structure is available!

But what exactly is a computation's causal structure? It's commonly represented as a graph, where the nodes represent events (e.g. bit flips) and the directed edges represent causal dependence between events[fn:27]. This causal graph is invariant to changes in details like the physical properties of the computer, how information is encoded, and the order of causally-independent events[fn:12]. It is also objective: all observers of a computation will infer the same causal structure[fn:13].

#+CAPTION: A causal graph representing computation as a network of events and their dependencies
#+ATTR_HTML: :style width:60%;max-width:60%;margin-left:auto;margin-right:auto;display:block
[[../img/wolfram-causal-graph.png]]

* Causal Graphs Fail to Bind
:PROPERTIES:
:ID:       b447fdac-556d-40f3-bee2-bcb2ec0a5fce
:END:

We'll proceed with the assumption that a computation's causal graph contains an objective representation of its hypothetical conscious state (2.2). This means that any measurable objective property of the conscious state must be understandable in terms of its causal graph. We'll focus only on one such property, which is the grouping of multiple events into each moment of experience (1.3). Crucially, we'll find that the intrinsic structure of a causal graph cannot account for this objective grouping (2.3).

What would it mean for a computation's causal structure to objectively group several events into a moment of experience? We can rephrase this and ask: how can we determine which events are /subjectively experienced as /simultaneous/ using the causal structure alone? If we imagine the /experience/ of each lightning strike as being represented by different parts of the graph, *what graph structure tells us they are experienced simultaneously*?

A naive approach would be to assign a time to every event in the graph and then determine simultaneity based on an equivalence between these times (i.e. the times on the two lightning experiences should match). This approach doesn't work because there simply is no such global time as part of the graph's structure. All we have is an abstract representation of events and their causal dependence. These events are not embedded in some larger structure, nor do they carry some internal time value. We only have the topology of the graph to work with![fn:29]

A more promising approach might be to define some internal perspective in the graph and then define simultaneity relative to this perspective. This is a key idea in [[https://arxiv.org/abs/1310.1667][Observer-Centric Physics]] as well as [[https://www.wolframphysics.org/][Wolfram Physics]]. The issue with these approaches is they only sharply define simultaneity relative to a single node of the internal perspective. So, the entity that can subjectively experience the simultaneous events is itself just a bit flip! That's not a very rich perspective to take.

#+ATTR_HTML: :style width:60%;max-width:60%;margin-left:auto;margin-right:auto;display:block
[[../img/knuth-chain.png]]

A final approach is to make an appeal to complexity: maybe a sufficiently tangled causal graph will have an /emergent/ notion of simultaneity relative to some /rich/ internal perspective. This may be true, but will this kind of simultaneity be truly objective? I don't see how it could. I think there will always be some fuzziness in this emergent definition, leaving open the possibility for qualiascopes to disagree on objective facts.

One way to see this is as a bootstrapping problem. To group events together, we first need a reference frame from which simultaneity can be defined. But any non-trivial reference frame must /itself/ consist of many events grouped together! We have an infinite regress. To believe that a causal graph can objectively group events together is like believing these hands can draw themselves out of the void[fn:28]:

#+CAPTION: M.C. Escher's "Drawing Hands" - illustrating the bootstrapping problem of self-referential binding
#+ATTR_HTML: :style width:60%;max-width:60%;margin-left:auto;margin-right:auto;display:block
[[../img/escher-hands.jpg]]

My take-away is to reject the idea that causal graphs have the necessary structure to account for multiple events being objectively grouped to the same conscious experience. We've already seen that this claim (2.3) contradicts the assumption of the objectivity of conscious states (1.2). So, something has to give...

* Potential Resolutions

In presenting this argument to proponents of computation accounts for consciousness, I've seen a several interesting responses.

The first is to reject that conscious states have an objective nature (assumption 1.2). The keywords here are "illusionism", "eliminativist materialism", and consciousness being an "as-if" virtual property. On this I'll just say that my aim is to explain conscious experience, not explain it away.

Another take is to say that I dismissed emergence too quickly. Maybe looking for binding in causal graphs is like asking which atoms in you body are alive. It's simply the wrong level of description to find that property. As discussed in the previous section, I think that emergent objectivity is hard to imagine in this case. If you've found a way, please let me know!

The final take comes from constructivist philosophy, which takes the stance that the only computational languages can be used to build fundamental models of reality. From this point of view, there must be something wrong with the argument, since seems to rule out any computational description of binding. I think there may be a subtle middle ground where we can fully describe binding in computational terms, but can't instantiate it as a computation.

* Closing Thoughts
:PROPERTIES:
:ID:       f765cc2d-4734-4d29-b7c4-65feab366c01
:END:

It's not easy for me to conclude that computer's can't be conscious. On one hand, it aligns with my intuition that we should not be worried about GPUs suffering, for example. On the other hand, I find many of the arguments for computationalists theories of mind compelling.

If we do reject conscious computation, then we need a framework beyond computation to explain our own consciousness. This does not necessarily imply physics has non-computable properties[fn:14]. Instead, we may find that even perfect simulations fail to capture certain properties of the reality they are simulating. The [[https://en.wikipedia.org/wiki/Map%E2%80%93territory_relation][map is not the territory]], and maybe the "wholeness" in the territory gets inevitably lost in a computational map. Something like this seems to happen when we simulate quantum computers on traditional computers: the "wholeness" of the quantum state gets fractured in the simulation of that state. This fracturing comes at a cost: the simulation generally needs exponentially more resources than the quantum computer.

So why not just assert that our brain leverages some "wholeness" in physics (e.g. quantum entanglement) which classical computers don't have access to? This is the approach pursued by QRI, and I consider it a very worthwhile investigation. If true, it could provide a solution to the "binding problem"[fn:15] as well as explain why biological evolution favored bound conscious states: wholeness comes with a computational advantage similar (or identical) to the advantage we find in quantum computers.

Of course, there are also reasons to reject this approach. Some compiutationists have convinced themselves that, actually, the map /is/ the territory[fn:30]. Or, at least they no longer think the distinction is philosophically sound. As previously mentioned, this "constructivist turn" in the philosophy of mind asserts that the only meaningful languages we can use do describe /anything/ must be [[https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_mathematics)][constructive]]. This turns out to be equivalent to saying that all models of reality must be computable, and that referencing any property (e.g. "wholeness") beyond what can be computed is a form of sloppy thinking. They might explain the wholeness we see in quantum states as a property models made by observers embedded in a branching "multiway" computation[fn:16], not an property of reality itself.

Finally, maybe the objectivity of conscious states assumption should be discarded since it's not even clear that physical states can be objectively defined[fn:18]! So, why should we expect that for conscious states? This might then make this argument impotent for ruling-out conscious computation, but leaves many others [fn:3] which don't use the objectivity assumption.

* Acknowledgements

Thank you [[https://x.com/algekalipso][Andrés Gómez Emilsson]] @ [[https://qri.org][QRI]] for introducing me to these ideas. Thank you [[http://bach.ai][Joscha Bach]] for [[https://lu.ma/3gul33by][provoking]] me to write them down.

* Related

- [[https://qualiacomputing.com/2023/10/26/the-view-from-my-topological-pocket-an-introduction-to-field-topology-for-solving-the-boundary-problem/][The View From My Topological Pocket: An Introduction to Field Topology for Solving the Boundary Problem]]
- [[https://youtu.be/g0YID6XV-PQ?si=v9yFUN22dndeVcrO&t=319][Solving the Phenomenal Binding Problem: Topological Segmentation as the Correct Explanation Space]].
- [[https://opentheory.net/2024/06/a-paradigm-for-ai-consciousness/][A Paradigm for AI Consciousness – Opentheory.net]]
- [[https://www.lesswrong.com/s/gBSsjYmdB2E4B2ymj][Computational functionalism on trial]]
- [[https://www.physicalism.com/#6][Non-materialist physicalism: an experimentally testable conjecture.]]
- [[https://philsci-archive.pitt.edu/1891/1/UniverseCreationComputer.pdf][Universe creation on a computer]]

* Footnotes
:PROPERTIES:
:ID:       c34ddc64-5fc5-4f0f-9069-e5f23520a02f
:END:
[fn:31] From 2001: A Space Odyssey

[fn:17] https://en.wikipedia.org/wiki/Relativity_of_simultaneity
[fn:30] See [[https://arxiv.org/abs/2308.16068][Ruliology: Linking Computation, Observers and Physical Law]].
[fn:29] If this is confusing to you, don't feel bad. It literally took an Einstein to expell this notion of absolute time from physics! See the [[https://en.wikipedia.org/wiki/Relativity_of_simultaneity][relativity of simultaneity]].
[fn:28] This image is also used by Eric Weinstein as a key metaphor in Geometric Unity, but that's irrelevant to how it's used here.
[fn:27] The open philosophical debates about how to think about causality are not relevant here. There is no ambiguity about how to generate a causal graph from a computation.
[fn:26] Note that all of these applies to computation /in general/: we didn't make any assumptions about the type of computation or the computational substrate. So, even a perfect brain simulation is in question!
[fn:25] Any discrepancies between them should be attributed to measurement errors and/or faults in the devices. For simplicity we can assume that we have idealized qualiascopes, with no such errors.
[fn:24] According to ChatGPT o3: "the word 'qualiascope' likely originated with Logan Trujillo in 2003, but Giulio Tononi’s 2012 book popularized it so thoroughly that many non-specialists now attribute the term to him."
[fn:23] Though a panpsychist perspective may argue the rock has some minimal conscious experience.
[fn:22] [[https://youtu.be/zuZ2zaotrJs?si=_Y2Tyiz3_CrS-K2E&t=356]["The brain is a biological computer" - Ilya Sutskever]]
[fn:21] [[https://danfaggella.com/worthy/][A Worthy Successor - The Purpose of AGI - Dan Faggella]]
[fn:20] [[https://en.wikipedia.org/wiki/Orchestrated_objective_reduction#Microtubule_computation][Microtubule Computation - Wikipedia]]
[fn:19] [[https://www.washingtonpost.com/news/capital-business/wp/2017/10/09/think-humans-are-superior-to-ai-dont-be-a-carbon-chauvinist/][Think humans are superior to AI? Don’t be a ‘carbon chauvinist’ - The Washington Post]]
[fn:1] By "computer", I mean [[https://plato.stanford.edu/entries/turing-machine/][Turing Machines]] and their close cousins. This includes CPUs and GPUs, but doesn't include quantum computers.
[fn:2] "Consciousness" in this post it defined as "what it's like" to be like to be something. See intro [[https://proteanbazaar.substack.com/p/consciousness-actually-explained][here]].
[fn:3] Scott Aaronson has [[https://scottaaronson.blog/?p=1951][aggregated many other arguments]] against consciousness being a type of computation. My favorite is the question of whether an encrypted form of a computation can be conscious, since it looks random to anyone without the key!
[fn:4] I believe David Pearce was the first to make Andrés @ QRI aware of this argument.
[fn:5] From the [[https://qri.org/glossary#binding][QRI Glossary]]: "Global binding refers to the fact that the entirety of the contents of each experience is simultaneously apprehended by a unitary experiential self..."
[fn:6] This theoretical version of computational functionalism is discussed in [[https://www.lesswrong.com/posts/dkCdMWLZb5GhkR7MG/do-simulacra-dream-of-digital-sheep][Do simulacra dream of digital sheep?]].
[fn:7] A perfect simulation assumes sufficient computational resources and perfect knowledge of initial conditions (practically impossible). It must compute the same transformations on (representations of) physical states that we expect from reality (i.e fundamental physicical laws). Our present understanding of quantum theory restricts such simulations to only producing outcome probabilities for a given measurement frame.
[fn:8] This reasoning doesn't imply that near-term AI systems will be conscious - it just suggests that computers aren't missing something fundamental to support consciousness.
[fn:9] Watch [[https://en.wikipedia.org/wiki/Pantheon_(TV_series)][Pantheon]].
[fn:10] Otherwise the qualiascopes measuring a computer would trivially always agree (with no output).
[fn:11] See [[https://plato.stanford.edu/entries/multiple-realizability/][multiple realizability]] and [[https://www.edge.org/response-detail/27126][substrate independence]].
[fn:12] Permutation City by Greg Egan takes this concept to a beautiful extreme, demonstrating the absurd conclusions one must accept under computational accounts for consciousness.
[fn:13] Though the recent no-go condition from the Extended Wigner's Friend experiment leaves open the possibility of observers of quantum systems disagreeing on causal structure. See [[https://www.wignersfriends.com/][We should run Wigner's Friend experiments]].
[fn:14] Non-computable physics being necessary to explain consciousness was famously proposed by Roger Penrose in [[https://en.wikipedia.org/wiki/The_Emperor%27s_New_Mind][The Emperor's New Mind]].
[fn:15] See the "Binding/Combination Problem" or the "Boundary Problem". See Chalmer's exposition [[https://consc.net/papers/combination.pdf ][here]].
[fn:16] This is the approach taken by [[https://www.wolframphysics.org/][Wolfram Physics]], which models both minds and their environments as computations that are continuously branching and merging in a computational multiverse. The wavefunction (and its collapse) are not part of the ontology, but instead just a tool used by observers to make predictions in this multiverse.
[fn:18] [[https://g.co/kgs/6bUpuYX][Trespassing on Einstein's Lawn]] is a beautiful account of this idea.

* COMMENT TODO

- try again with one qscope that has full access to the complete description of the physical state / computation.
- mention consciousness as a process doesn't fix it
- use the fact that computational events are onotologically distinct -> clean causal graph
- qscopes measure bits, and can only agree on inference of objective causal structure of the computation that generated those bits. this is not the case in a more general physical case, where the bits generally come from wavefunction collapse
- img captions / credit


* COMMENT Send to

  Adam
  Creon
  will m
  will z
  yudhi
  Andres
  M Johnson
  Murat
  Franz
  hikari
  W
  Miron
  Dad
  nik
  leona
  liza
  luca
  felix
  sat
  biz
