---
title: Don't Cry for HAL (WIP)
date: 2025-05-29
---

** Introduction

I don't think that computers [fn:7] will ever wake up, no matter what software they are running.

This is a very difficult position to defend, including to myself. In this post, I'll present the most compelling argument I've heard for this stance, which I first learned from [[https://qri.org/][QRI]].

** A Case for Conscious Computers

Computational accounts for consciousness are [[https://cimc.ai/][in vogue]].  Their detractors are usually characterized as scientifically illiterate, "carbon chauvinists", and/or microtubule-obsessed. What makes the proponents so sure?

One solid way to conclude that a computer could /in principle/ be conscious goes like this [fn:16]:

1. Consciousness [fn:21] is generated by the brain.
2. Physical systems like the brain can be perfectly simulated [fn:4] on a computer.
3. A perfectly-simulated brain would have the same properties as the original, consciousness included.

There are strong reasons to believe in each of these and they imply that a powerful-enough computer, running the right program, will wake up. This reasoning doesn't imply that near-term AI systems will be conscious - it just suggests that computers aren't missing something fundamental to support consciousness.

This conclusion is deeply seductive. If true, we can understand the brain as a [[https://youtu.be/zuZ2zaotrJs?si=_Y2Tyiz3_CrS-K2E&t=356][biological computer]]. If true, we can look forward to uploading (a copy of) our minds onto immortal substrates [fn:18]. And, if true, we might build a conscious AI that loves humanity.

The present argument against conscious computers takes aim at #3 above and suggests that even a perfect brain simulation won't wake up.

** An Overview of the Argument
:PROPERTIES:
:ID:       3b9d3431-9e6c-4a3f-8d1a-5c903739cdcb
:END:

Most of the work is carefully specifying a set of assumptions about consciousness and computation. Once these are defined, it's straightforward to show they imply a contradiction .

The three assumption are:

1. *Conscious Computation*: certain computations have conscious experience as an intrinsic property.
2. *Substrate Independence*: all intrinsic properties of a computation can be derived from its /causal structure/.
3. *Objectivity of Conscious States*: the information content of a conscious experience is /objective/ [fn:19].

Together, they imply *the contents of a computation's experience must be objectively determined by the computation's /causal structure/.*

We'll see why this can't be true, implying at least one assumption is false.

** Unpacking the Assumptions

*** Conscious Computation

#+begin_quote
“Dave, stop. Stop, will you? Stop, Dave. Will you stop, Dave? Stop, Dave. I'm afraid. I'm afraid, Dave. Dave, my mind is going. I can feel it. I can feel it. My mind is going. There is no question about it. I can feel it. I can feel it. I can feel it. I'm a... fraid.” - HAL9000, 2001: A Space Odyssey
#+end_quote

The *Conscious Computation* assumption says that HAL (an AI) wasn't necessarily faking its feeling of fear. That is, it's in principle possible that HAL could have experienced its death, that maybe there's "something it's like" to have been HAL, and that our emotional response to the text is not necessarily misplaced.

Note that this assumption makes no restrictions on how complex the computation must be to support consciousness. A physics-perfect simulation of a brain is still fair-game.

*** Substrate Independence

Imagine you want to test how HAL would behave in a certain situation, so you run HAL in a virtual world with no interface to the physical world. Could HAL do an experiment in the virtual world to learn anything about the physical? For example, can it determine any physical properties of the computer it's running on?

*Substrate Independence* [fn:20] says the answer is "no" [fn:22]. The reason is that the HAL's experiments can only determine the /causal structure/ (defined later) of the computation and the same causal structure can be implemented by many different substrates (i.e physical systems). The causal structure captures all /intrinsic/ properties of a computation, pulling them away from details of how that structure is instantiated on a physical substrate.

We benefit from substrate independence all the time: it's what makes the concept of "software" meaningful. We generally write code without worrying about the layout of the transistors, the temperature of the chip, etc... As long as we have sufficient computational resources, we expect the code to always produce the same output. This works because the software specifies a causal structure that is independent of the substrate running it. If you think you have a counterexample, consider that the physical machine could itself be fully virtualized.

*** *Objectivity of Conscious States*

Consider the question: "what are you currently experiencing?".

The *Objectivity of Conscious States* assumption says that there is a /single/ correct answer to this question, independent of who is asking the question.

Under [[https://plato.stanford.edu/entries/physicalism/][Physicalism]], we could make this more precise:

Any conscious state is fully determined by a complete and objective descripiton of its underlying physical state. For example, a complete physical description of the brain, over some period of time, would leave no ambiguity about the corresponding conscious experience.

** The Contradiction

Taken together, these assumptions imply there should be a single objective answer to the question: "what is HAL currently experiencing"?

To answer it, we should start with the causal structure associated with HAL's software. Then, we should look for how that structure intrinsically selects a subset of the events as being part of the same experience. We might expect that subset to change as the computation progresses, but let's keep it simple for now and just expect at least one such subset (i.e. a minimal "moment of experience").

What is a causal structure, specifically? Imagine you could zoom into HAL's processor as it was running. You'd see digital memory rapidly changing state with a "bit flip" event being the most relevant primitive. You might also notice that the current state of memory determines the next state, with logical circuits implementing the transition. To abstract-away any details of the physical implementation, consider each bit flip event as a node in a graph. Then, add a directed edge between events to indicate causal dependence (i.e. a certain bit flip event could only logically occur after another). The resulting directed-acyclic causal graph represents the lowest-level causal structure of HAL's computation. This would have too many details to be useful in practice, but that's not relevant for the present argument.

What kind of substructure in this graph could correspond to a "moment of experience"? Minimally, it would need to associate many bit flip events to a single "frame" of HAL's subjective time. This is referred to as "binding" the information into a single experience. The lack of a consensus on a solution to this is called The Binding Problem [fn:12].

There are only a few ways I could imagine defining intrinsic binding in a causal graph. Most obvious is to just assert that when many events are in the causal past of a single event, they are all "bound". This fails because the single event has no internal structure to integrate the information - it's just a bit flip!

Another approach might be to define an extended "screen" of events, and define all the events impinging on the screen to bound into the same experience. This fails because trying to define the screen generates an infinite regress: what intrinsic structure in the graph would select out the events corresponding to the screen? That's the problem we set out to solve!

The last option is to say the binding is /emergent/ in some tower of complexity and abstraction built on top of the graph. Computationalists talk about things like recursion, phase-locking, self-modeling, attention-heads, second-order perception, prediction error minimization, integrated information, etc... They expect that somewhere in these abstractions built from the causal graph, a well-defined "moment of experience" will arise.  I can't see how that could be true, given that the underlying causal graph is missing the relevant structure to define objective boundaries / subsets.

My conclusion from this argument is to reject the *Conscious Computation* assumption and answer "nothing" to "what is HAL currently experiencing".

** Discussion

- Implications for models of consciousness - may need binding in the ontology.
- Mention that some computationalists may reject the objectivity assumption instead.
- Ruliadists take the position that map=territory, hence everything is computer.
  - That's going too far. This argument shows we should be more humble about the territory.
- Address the "wrong level of description" counterpoint (e.g. which atoms in the body are alive)

** Acknowledgements

Thank you [[https://x.com/algekalipso][Andrés Gómez Emilsson]] @ [[https://qri.org][QRI]] for introducing me to these ideas [fn:2]. Thank you [[http://bach.ai][Joscha Bach]] for provoking me to write them down.

Thank you Franz, Hikari, Lou, Teafaerie, and Theda for helpful discussions!

** Footnotes
:PROPERTIES:
:ID:       c34ddc64-5fc5-4f0f-9069-e5f23520a02f
:END:
[fn:23] [[https://g.co/kgs/6bUpuYX][Trespassing on Einstein's Lawn]] is a beautiful account of this idea.
[fn:22] Technically, HAL can confirm that it's running on a Turing-complete substrate, but that's it.
[fn:21] Defined here as "what it's like" to be something (see intro [[https://proteanbazaar.substack.com/p/consciousness-actually-explained][here]]). This does not necessitate a sense of self.
[fn:20] Max Tegmark presents consciousness as second-order substrate-independence in [[https://www.edge.org/response-detail/27126][this Edge essay]].
[fn:19] This corresponds to Camp #2 in [[https://www.lesswrong.com/posts/NyiFLzSrkfkDW4S7o/why-it-s-so-hard-to-talk-about-consciousness][Why it's so hard to talk about Consciousness — LessWrong]]
[fn:18] Watch [[https://en.wikipedia.org/wiki/Pantheon_(TV_series)][Pantheon]].
[fn:16] This theoretical version of computational functionalism is discussed in [[https://www.lesswrong.com/posts/dkCdMWLZb5GhkR7MG/do-simulacra-dream-of-digital-sheep][Do simulacra dream of digital sheep?]].
[fn:17] Our Mathematical Universe: My Quest for the Ultimate Nature of Reality
[fn:15] You might expect at atom to be one of the simplest things to simulate, but it's not! Even perfectly simulating a hydrogen atom in its ground state is a formiddable task for a classical computer!
[fn:14] Scott Aaronson has said the real thing to explain is the Classical Slowdown, not the Quantum Speedup. This is because quantum computers run at the same "speed level" as the underlying reality, where normal computers suffer an exponential slowdown.
[fn:3] Which leads some people, like Seth Lloyd, to declare that the universe /is/ a quantum computer.
[fn:7] By "computer" I mean a classical computer like today's digital computers (e.g. CPUs and GPUs), often understood to mean a fancy Turing Machine. Quantum computers are something else.
[fn:13] For example, Integrated-information Theory (IIT) provides a metric for how conscious a system that can be computed from the graph's structure. However, it doesn't identify an intrinsic mechanism for determining why a system like the brain generates on unified experience instead of many smaller ones.
[fn:11] Scott Aaronson aggregated additional examples [[https://scottaaronson.blog/?p=1951][here]] of the absurd conclusions that computational theories of mind lead to.
[fn:1] Permutation City by Greg Egan takes this concept to a beautiful extreme, demonstrating the absurdity of computational theories of mind.
[fn:10] This is the approach taken by [[https://www.wolframphysics.org/][Wolfram Physics]], which models both minds and their environments as computations that are continuously branching and merging in a computational multiverse. The wavefunction (and it's collapse) are not part of the ontology, but instead just a tool used by observers to make predictions in this multiverse.
[fn:2] See [[https://qualiacomputing.com/2023/10/26/the-view-from-my-topological-pocket-an-introduction-to-field-topology-for-solving-the-boundary-problem/][The View From My Topological Pocket: An Introduction to Field Topology for Solving the Boundary Problem]] and [[https://youtu.be/g0YID6XV-PQ?si=v9yFUN22dndeVcrO&t=319][Solving the Phenomenal Binding Problem: Topological Segmentation as the Correct Explanation Space]].
[fn:12] Also called the "Combination Problem", see Chalmer's exposition [[https://consc.net/papers/combination.pdf ][here]].
[fn:4] A perfect simulation assumes sufficient computational resources and perfect knowledge of initial conditions (practically impossible). It must compute the same transformations on (representations of) physical states that we measure in reality. Quantum theory restricts such simulations to only producing outcome probabilities for a given measurement frame.
[fn:5] Joscha Bach says that for something to exist it must be implemented, and that therefore only computational/constructive languages should be used in modelling fundamental physics. [[https://www.wolframphysics.org/][Wolfram Physics]] is one notable effort in this direction.
[fn:6] Non-computable physics being necessary to explain consciousness was famously proposed by Roger Penrose in [[https://en.wikipedia.org/wiki/The_Emperor%27s_New_Mind][The Emperor's New Mind]]. I'm /not/ saying this.
[fn:8] It's not clear how the brain could make use of this wholeness in physics, but at least it's possible. Computers making use of it seems impossible by construction.
[fn:9] David Bohm named wholeness as the hallmark of quantum theory in "[[https://en.wikipedia.org/wiki/Wholeness_and_the_Implicate_Order][Wholeness and the Implicate Order]]"

** Related

- [[https://opentheory.net/2024/06/a-paradigm-for-ai-consciousness/][A Paradigm for AI Consciousness – Opentheory.net]]
- [[https://www.lesswrong.com/s/gBSsjYmdB2E4B2ymj][Computational functionalism on trial]]
- [[https://www.lesswrong.com/posts/kd37DZftKLDguqtKr/a-review-of-don-t-forget-the-boundary-problem][A review of "Don’t forget the boundary problem..." — LessWrong]]
- [[https://proteanbazaar.substack.com/p/consciousness-actually-explained][Consciousness Actually Explained: EC Theory - by Casey]]


* COMMENT WIP

** Causal Structure...

** Conclusions

** Open Questions

Before diving into the full argument, let's clarify some points.

*** How can a perfect simulation of a brain not be conscious? Wouldn't that contradict the term "perfect"?

If a perfect simulation was missing some property of the original system, then that property must have no effect on the output (otherwise it wouldn't be a /perfect/ simulation). By this reasoning, it follows that such a property, if it has any meaningful existence at all, can be safely ignored.

Hold on! There's a difference between "no effect on the output" and "no effect at all". What if the missing property only affected /how/ the information in physical states is transformed? So, the simulation would still arrive at the same final state, but not by processing information in the same way as the original. If that's the case, and you were in the simulation, could you tell the difference? Does it matter how you're processed?

#+begin_quote
I think that consciousness is the way information feels when being processed in certain complex ways.
Max Tegmark [fn:17]
#+end_quote

If you think consciousness is a computation, then you must also think it matters how the information is processed in that computation. Why? Imagine you had all possible input-output combinations of a conscious computer. You could then replace that computer with a very big lookup table, which implements the same function as the original (in some sense, it's the same computation). Nobody would claim that a lookup table is conscious - it must be something about /how/ the function is computed.

So, even a perfect simulation of a conscious brain may not itself be conscious. It's possible that there's something in /the way in which/ physics processes information that can't be simulated.

*** What exactly is a "causal structure"? Why is it a complete objective representation of a computation?

The first concept we need is [[https://www.edge.org/response-detail/27126][substrate-independence]], which generally comes along with computational theories of mind.  It means that only the causal structure is relevant "from the inside" of a computation, not the details of the physical implementation. Imagine an AI embedded in a virtual world running on a GPU. From the perspective of that AI, it can't tell what the GPU is made of, how fast it's running, or in what order causally-independent steps are performed [fn:1]. It also wouldn't notice if it was migrated from a GPU to a CPU (or even a [[https://www.youtube.com/watch?v=vo8izCKHiF0][wooden Turing Machine]]), so long as the causal structure of the program was perfectly maintained [fn:11].

But what exactly is this "causal structure"? The causal structure for any computation can be represented as a directed acyclic graph. Each node corresponds to a bit flip event. The edges represent a direct causal dependence between events. This "causal graph" abstracts away the physics of the computer and only keeps the structure that relates outputs to inputs, forming a complete and substrate-independent representation of the computation.

*** In what sense it the brain not a computer? If this argument is correct, how can the brain be conscious? How does it avoid the same conclusion?


*** [[https://www.youtube.com/watch?v=5uPkOLr7Yjs][Everything is Computer]]?

I think the fundamental laws of physics must be computable [fn:5]. That means, given a powerful enough computer, reality can be perfectly simulated [fn:4].

Then why don't I think a perfectly-simulated brain would wake up? Well, the [[https://en.wikipedia.org/wiki/Map%E2%80%93territory_relation][map is not the territory]]. Just because physics can be perfectly /modeled/ computationally, that doesn't mean that reality /is/ a computer. This is a subtle point. I'm not saying that physics has some non-computable elements [fn:6]. I'm saying that even a perfect simulation of a physical system can fail to retain all of the properties of the actual system. And yes, I mean even if you take a perspective /from the inside/ of the simulation, it's not the same as the original.

Everything is computable, but everything is /not/ (a) computer!

** Unity in Mind, Physics, and Computation

Our awareness has rich structures in it, bound together in a [[https://plato.stanford.edu/entries/consciousness-unity/#OneFirVie][unified whole]]. We experience a "now" with many things happening "all at once". This property is referred to as "global binding" and it must be explained by any account of conscious experience. The present lack of such explanation is called the "Binding Problem" [fn:12] and, more clearly than the [[https://iep.utm.edu/hard-problem-of-conciousness/#:~:text=The%20hard%20problem%20of%20consciousness%20is%20the%20problem%20of%20explaining,directly%20appear%20to%20the%20subject.][Hard Problem]], I think it puts [[https://plato.stanford.edu/entries/computational-mind/][computational theories of mind]] at a loss relative to some other approaches, like [[https://qri.org/blog/taking-monism-seriously][Dual-Aspect Monism]].

Modern physics seems well-equipped to account for the wholeness of experience [fn:9]. Entangled quantum states are not separable into distinct parts. Wavefunction collapse is non-local and acausal. The phenomena of superconductivity and superfluidity are characterized by particles losing their individual identities and fusing into coherent wholes [fn:8]. The current point is not to assert that any of these specific processes must be leveraged in conscious brains. For now just notice that, as an ontology, physics has the necessary ingredients for unity.

Computational models, on the other hand, provide no such ingredients. They encode states into distinct parts (e.g. bits), and all of their dynamics (state transitions) preserve this initial separation. Case closed.

** Preempting Joscha

I can actually hear the screams of the computationalists as I write this.

One objection is that "physical wholeness" is just what it looks like /from the inside/. A mind running on simulated physics could deduce the same laws of physics, and therefore has no way to determine whether it's running in a simulation vs "basement reality". The wholeness we see in quantum mechanics could be fully explained as properties of the models such a mind makes under necessarily incomplete information [fn:10].

This objection is valid: it's insufficient to point to /apparent/ wholeness in physics to rule out computational theories of mind. But that's not what's happening. I'm pointing to the wholeness of direct experience and will show that computational ontologies can't account for it. The fact that physics provides a more suitable ontology is important, but not the main point.

The computationalist's second objection is that any model of reality/experience must use the language of constructive mathematics, which is equivalent to computation. So, as soon as we want to meaningfully describe "wholeness" or "binding", we will find ourselves forced to use computational terms (e.g. sets, graphs, state transition functions). If anyone's ever condescendingly accused you of making the mistake of using "stateless mathematics" in your foundations, this is the stance they were taking.

Again, I think this objection is both valid and misses the point. I don't think modeling wholeness necessitates using a continuum or any other non-constructive mathematical concept. As already mentioned, there are good reasons to expect physics (with all of its wholeness) to be computable. This objection is just the same logical fallacy of mistaking the map for the territory. I take the wholeness of my experience as a given and seek a model that can explain it. The fact that any viable model is computable does /not/ imply that a computer can have a unified experience. I didn't appreciate this point until I understood why computation can't account for binding.

** Computers Can't Bind

Maybe looking for wholeness in the ontology is a mistake. What if wholeness is a virtual property? Could there be a computational mechanism that generates unity and provides an account for the wholeness of our experience? A careful consideration of the nature of computation rules this possibility out.


Now we can make a key claim:

Any computationalist account for the experience of a unified "now" must (minimally) provide a rule that identifies which events in a causal graph are part of the same "now". Critically, this rule must only use the intrinsic structure of the causal graph.

I'm convinced this is impossible, even when allowing for arbitrarily complex abstractions implemented on top of the causal graph. To see why, consider what structure is available to define such a rule. I only see two possibilities.

First, imagine the case where many events are the direct cause of a single downstream event. Then, from the perspective of that single event, there's a precise way to group all of the upstream events into a single "now". There are obvious issues with this approach - it's only given as an example of what a rule could look like. One issue is that the "binding" occurs at a single node, which corresponds to a single bit flipping. It goes without saying that a single bit is insufficient to implement conscious awareness...

The other possibility is that causality itself provides the binding, so /any/ events that are causally related should be identified with the same "now". This is equivalent to saying that you're currently experiencing your entire past light cone. As Andrés says, this approach fails to generate a boundary in time. Someone once told me they believe the finite computational resources of the observer can generate this boundary, restricting the experience to just the tip of the light cone. I don't see any way to make sense of that idea.

Causal graphs simply don't have the right ingredients to construct an extended perspective that's experiencing a "now".

** Conclusion


** Appendix
:PROPERTIES:
:ID:       e036dca6-117d-496e-8810-5a57d3ba8c95
:END:

*** Steelmanning Computational Functionalism

TODO

- for something to exist it must be implemented
- a thing is defined by its behaviors, has no "essence" outside of what it does
- computation is equivalent to constructive mathematics
  - this is the subset of mathematics that can be used to make models without allowing nonsense/contradictions
  - implication -> fundamental physics should be modeled computationally
- understand consciousness to be a virtual property
  - the brain is physical and therefore not conscious
  - only a simulated "you" within a simulated "world" can be conscious
- substrate-independence implies that a GPU running the same causal structure as a conscious brain will also wake up -> conscious AI is possible and mind uploading should work in principle.

**


*** Taking the Now for Granted

I think many people miss the fact a "now" needs to be implemented at all. The logic goes like this:

1. Reality/physics already has a "now".
2. Computers and brains are implemented on top of physics.
3. The experienced "now" therefore naturally inherits from the underlying physical "now".

This argument sets aside substrate-independence, since it's assuming a connection between the physical and virtual that's not captured in the causal graph. But it's worse than that...

Reality doesn't actually have a single objective "now" that's extended over space. While unintuitive, this is a foundational concept in modern physics and called the "[[https://en.wikipedia.org/wiki/Relativity_of_simultaneity][relativity of simultaneity]]". It says that observers in different inertial reference frames will disagree on what events are simultaneous. This fact does not prevent systems like brains or GPUs from being functionally synchronized or coherent: the speeds and distances are too small. However, this misses the point: how can we define the reference frame /itself/ as an extended object? We either end up with circular logic, or rely on the simultineity of an observer's conscious experience (which is what we're trying to explain!).

This is a subtle but important point, so I'll say it another way:

There's an objective fact as to which events an observer labels as simultaneous in their experience. However, there is no objective fact as to which physical events are simultaneous in the implementation of that same observer. This discrepancy in objectivity implies that physical simultaneity is a bad explanation for experienced simultaneity.

*** No objectivity

What would it mean to drop this assumption? Well many aspects of physics are actually observer-dependent [fn:23], not objective. For example, there's no objective sense that two events happen "at the same time". Even the number of particles in a region of space will [[https://en.wikipedia.org/wiki/Unruh_effect][change]] depending on the measurement frame. Why then would conscious states be objective?
